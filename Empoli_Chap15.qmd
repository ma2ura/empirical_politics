# ロジスティック回帰分析

```{r}
#| echo: FALSE
library(tidyverse)
library(ggthemes)
mystyle <- list (#  ggplotのテーマ
  theme_few(), # ggthemesパッケージ
  theme(
    text = element_text(
      size=10,  #  フォントサイズ
     family = "HiraKakuProN-W3" # ヒラギノフォント
    )
  )
)
set.seed(123)
```

## ロジスティック関数

「当たったか、外れたか」、「ある会計基準を選択したか、否か」、「ある商品を購入したか、否か」など、結果が二値で表されるような変数を二値変数(binary variable)といい、二値変数を応答変数として回帰分析したいとき、ロジスティック回帰分析が便利です。

事象Aと事象Bのどちらかが起こるとき、事象Aが起こる確率を$p$とすると、事象Bが起こる確率は$1-p$となります。
$p$は確率を表しているので，$0$から$1$の間の値をとります。
この事象Aが起こる確率と事象Aが起こらない確率の比を**オッズ(odds)**といいます。

$$
\frac{p}{1-p}
$$

このオッズは，$0$から$\infty$の間の値をとります。
たとえば事象Aが起こる確率が10％と見積もられる場合，オッズは$0.1/0.9 = 0.111$となります。
さらにこの**オッズを対数変換**して，$p$の関数$f(p)$としたものをロジット関数と言います。
こうすることで，$p$は$0$から$1$の値をとるとき，$f(p)$は$-\infty$から$\infty$の値をとるようになります。

$$
f(p) = \log \left( \frac{p}{1-p} \right) = \log p - \log (1-p)
$$

図で書くとこうなります。

```{r}
#| echo: FALSE
#| fig.cap: "ロジット関数"
#| fig.height: 6
#| fig.width: 6
p <- seq(0, 1, 0.005)
logit <- log( p / (1 - p))
df <- data.frame(p, logit)
g <- ggplot(df) + aes(x = p, y = logit) + geom_line() # 折れ線グラフ
g <- g + geom_hline(yintercept = 0, linetype = "dashed") + geom_vline(xintercept = 0.5, linetype = "dashed")
g <- g + labs(x = "確率p", y = "ロジット関数f(x)") + mystyle
print(g)
```

次に，ロジット関数$f(p)$の逆関数を考えます。
対数関数の逆関数は指数関数になるので，先のロジット関数の両辺の指数をとると，次のようになります。

$$
\exp(f(p)) = \frac{p}{1-p}
$$

この式を$p$について解くと，次のようになります。

$$
\begin{aligned}
p &= \frac{\exp(f(p))}{1 + \exp(f(p))} \\
  &= \frac{\frac{\exp(f(p))}{\exp(f(p))}}{\frac{1 + \exp(f(p))}{\exp(f(p))}} \\
  &= \frac{1}{\frac{1}{\exp(f(p))}+1} \\
  &= \frac{1}{1 + \exp(-f(p))}
\end{aligned}
$$

ロジット関数$f(p)$の逆関数を$f^{-1}(x)$とすると，$f^{-1}(x)$は$-\infty$から$\infty$の値をとるとき，$p$は$0$から$1$の値をとるようになります。
これからの分析に必要な関数の形がでてきました。
この関数$f^{-1}(x)$を標準ロジスティック関数と言います。

$$
f^{-1}(x) = \frac{\exp(x)}{1 + \exp(x)} = \frac{1}{1 + \exp(-x)}
$$

標準ロジスティクス関数は次のような形をしています。

```{r}
#| echo: FALSE
#| fig.cap: "標準ロジスティック関数"
#| fig.height: 6
#| fig.width: 6
#| fig.align: "center"
#| fig.asp: 1
#| fig.show: "hold"
#| fig.lp: "fig:logistic"
#| fig.env: "figure*"

x <- seq(-6, 6, 0.005)
logistic <- exp(x) / (1 + exp(x))
df <- data.frame(x, logistic)
g <- ggplot(df) + aes(x = x, y = logistic) + geom_line() # 折れ線グラフ
g <- g + geom_hline(yintercept = c(0,1), linetype = "dashed")
g <- g + labs(x = "x", y = "標準ロジスティック関数") + mystyle
print(g)
```

標準ロジスティクス関数の定義域は$-\infty$から$\infty$ですが，$x$が$0$のとき，$f^{-1}(x)$は$0.5$となります。

応答変数が二値変数となる場合の分析手法で最もよく利用されているものが，ロジスティック回帰分析です。
手元の応答変数データは$0$と$1$の2種類しかなく、このようなデータを生み出す確率モデルにはベルヌーイ分布が適しています。
ベルヌーイ分布は、確率$p$で$1$、確率$1-p$で$0$をとる確率分布です。
この確率$p$を先ほど導出したロジスティック関数(logistic function)で表します。


$$
\text{logistic}(x) = \frac{\exp(x)}{1 + \exp(x)} = \frac{1}{1 + \exp(-x)}
$$

このロジスティック関数を使って、確率$p$を次のように表すことができます。

$$
\Pr(y_i = 1)  = \text{logistic}(b_0 + b_1x_i) =  \frac{1}{1 + \exp(-\beta_0 - \beta_1 x_i)}
$$

この式は、$x_i$が与えられたときに$y_i$が$1$となる確率を表しています。
この式を変形すると、次のようになります。

$$
\log \left( \frac{\Pr(y_i = 1)}{1 - \Pr(y_i = 1)} \right) = \beta_0 + \beta_1 x_i
$$

ようやく回帰分析の式になりました。

### 最尤法

つぎに，この$\beta$を推定する方法を考えます。
この回帰モデルは非線形であるため，モデルと観測値の誤差を最小にする，という最小二乗法を使ってパラメータを推定することはできません。
そこで，**最尤法**(most likelifood method)を使ってパラメータを推定します。
最尤法とは，観測値が得られる確率を最大にするようなパラメータを推定する方法で，一定の条件のもとで優れた推定量を与えることが知られています。

最尤法について考える前に，まずロジスティック回帰のモデルの背後にある線形モデルについて考えてみます。
観察される応答変数$y_i$は$0$か$1$という二値変数となりますが，その背後には，線形関係があると考えることができます。
つまりある閾値$y^*$を設定して，$y_i$が$y^*$より大きいときは$1$，$y^*$より小さいときは$0$となると考えることができます。

$$
y_i =
\begin{cases}
1 & \text{if } \beta_0 + \beta_1 x_i + \epsilon_i > y^* \\
0 & \text{if } \beta_0 + \beta_1 x_i + \epsilon_i \leq y^*
\end{cases}
$$

## ロジスティック回帰式の推定

実際にRでロジスティック回帰分析を行う場合は，線形回帰モデルの推定で利用した`lm()`関数ではなく、`glm()`関数を使うだけで，ほぼ同じように分析できます。
ここでは、不正を行う企業の特徴を分析してみます。

```{r}
df <- read_csv("data/fraud.csv") %>%
  select(SCODE, Year,NKILM, FEM_NUM, BOARDSIZE, IDOUT_NUM,AGE_AVERAGE, ROA, DASS, FRAUD)
df$Year <- as.factor(df$Year)
df$NKILM <- as.factor(df$NKILM)
glimpse(df)
```
ここで、分析に使う変数は次のものです。


- `SCODE`は証券コード
- `Year`は年度
- `FEM`は女性役員数
- `BOARDSIZE`は取締役会の人数
- `IDOUT_NUM`は独立役員数
- `AGE_AVERAGE`は取締役会の平均年齢
- `ROA`は総資産利益率
- `DASS`は負債比率
- `FRAUD`は不正を行った企業ならば1、そうでなければ0

検証する仮説は「**外部取締役が多いほど、会計不正が発生しない**」というものです。
この仮説を検証するために、ロジスティック回帰分析を行います。
ここでは、ロジスティック回帰分析を学習するために、あえて少ない変数で分析を行います。
$$
% \begin{aligned}
FRAUD_{it} = \beta_0 + \beta_1 BOARDSIZE_{it} + \beta_2 AGE\_AVERAGE_{it} + \varepsilon_{it}
% \end{aligned}
$$

これを推定するために、`glm()`関数を使います。
`glm()`は一般化線形モデルを推定するための関数で、引数として、`formula`、`family`、`data`を指定します。ここでは、`formula`には推定する式を、`family`には`binomial(link = "logit")`を指定します。`binomial`は二項分布を指定するオプションで、リンク関数としてロジスティック関数の逆関数であるロジット関数を指定しています。`data`には、分析に使うデータフレームを指定します。


```{r}
model_1 <- glm(FRAUD ~ BOARDSIZE + AGE_AVERAGE,   
               family = binomial(link = "logit"), data = df)
summary(model_1)
```

有意な変数がほとんどありません。
強いて言うなら、負債比率が高いほど、不正が発生しやすいという結果が得られました。

